{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_practice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmcD9tBFiUhK",
        "outputId": "af06c4ab-ec45-4c72-9c8c-6ae0425085c3"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 55.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAeUm9lziyRC"
      },
      "source": [
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from tokenizers.normalizers import BertNormalizer\n",
        "\n",
        "train_data = '/content/drive/MyDrive/History_SentenceMaker/history_data/history_sentence.txt'\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n",
        "handle_chinese_chars=False,\n",
        "lowercase=False)\n",
        "\n",
        "tokenizer.train(\n",
        "    train_data,\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\",]\n",
        ")\n",
        "\n",
        "tokenizer.save_model(\"/content/drive/MyDrive/History_SentenceMaker/model\")\n",
        "tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"/content/drive/MyDrive/History_SentenceMaker/model/vocab.json\", merges_filename=\"/content/drive/MyDrive/History_SentenceMaker/model/merges.txt\")\n",
        "\n",
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n",
        "tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
        "tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
        "tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
        "tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xHtUQikkuVa"
      },
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "# creating the configurations from which the model can be made\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.get_vocab_size(),\n",
        "  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n",
        "  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
        ")\n",
        "# creating the model\n",
        "model = GPT2LMHeadModel(config)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0edfP-pnJiq"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers.utils import logging\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        cache_dir: Optional[str] = None,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            cache_dir if cache_dir is not None else directory,\n",
        "            \"cached_lm_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Make sure only the first process in distributed training processes the dataset,\n",
        "        # and the others will use the cache.\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "        with FileLock(lock_path):\n",
        "\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                self.examples = []\n",
        "                text = \"\"\n",
        "                with open(file_path, encoding=\"utf-8\") as f:\n",
        "                    lines = f.readlines()\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가\n",
        "                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합친다\n",
        "                tokenized_text = tokenizer.encode(text).ids\n",
        "\n",
        "                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장\n",
        "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                    self.examples.append(\n",
        "                        tokenized_text[i : i + block_size]\n",
        "                    )\n",
        "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
        "                # If your dataset is small, first you should look for a bigger one :-) and second you\n",
        "                # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> torch.Tensor:\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_data,\n",
        "    block_size=32,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,# GPT는 생성모델이기 때문에 [MASK] 가 필요 없음\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "HM8dvw5UnRYU",
        "outputId": "eb68c1c1-c026-488b-ab34-993607f4f8ac"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=100,\n",
        "    save_total_limit=5,\n",
        "    logging_steps=100\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2250' max='2250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2250/2250 11:05, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.137200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.122400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.100900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.094900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.090800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.087400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.084100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.079600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.078900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.076300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.074500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.072500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.070500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.072800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.074700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.075100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK5ZJJOuow4B",
        "outputId": "7f61913b-e203-4e55-8c10-05bba4387b71"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"<s>\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=10)\n",
        "print()\n",
        "\n",
        "for generated_sequence in output_sequences:\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    print(tokenizer.decode(generated_sequence, skip_special_tokens=True))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "세종 재위 시기에는 우리 풍토에 맞는 농법을 소개한 『농사직설』이 간행되었다.\n",
            "청·프 저쟁이 벌어진 틈을 타 정변을 일으켰다는 것을 통해 갑신정변임을 알 수 있다.\n",
            "문벌귀족은 정치적 특권으로 음서제를 문벌귀족은 경제적 특권으로 공음전을 묘청 특권범전을 음서론화 받제를 문묘청천벌전을 묘청사관과 대화 공음전을 경제변청의 경제청 때 사청을 묘청 4에서는 벌화경화 부어사의 시행하였다.현산서 경제적 정묘과 후흥통령\n",
            "공민왕은 정동행성 이문소를 폐지하였다.\n",
            "신라 성덕왕은 백성에게 정전을 지급하였다.\n",
            "순종의 인산일을 기회로 전개하였다.\n",
            "대한민국 임시 정부가 조소앙의 삼균주의에 바탕을 둔 건국 강령을 발표한 것은 194 『아 보탐앙의 지계한 것은 남령왕 단하으며, 삼획 기무원용 단독립의 군사동 최주 강식, 미령 유일 대원군 나합령을 줄재금지방을 발표산성적주의와 지청제복무 국안을 지\n",
            "성왕은 수도를 사비로 천도하고 국호를 남부여로 고쳤다.\n",
            "신라 성덕왕은 백성에게 정전을 지급하였다.\n",
            "선농단은 조선 시대와 중국에서 농사와 인연이 깊은 신농씨와 후직씨를 주신으로 모씨 무씨직씨를 저술하였법 왕 과관을판조과 주신으로 모계국 발해계왕된 사건이다.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}